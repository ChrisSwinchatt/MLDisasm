#!/usr/bin/env python3

'''Usage: {0} <model>
'''

import os
import random
import sys
import traceback as tb
import warnings

import numpy as np

# Filter out debug messages from TF.
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

# Ignore warnings generated by using a different NumPy version.
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

if __name__ == '__main__':
    print('*** Starting up...')

import tensorflow               as tf
import tensorflow.keras         as keras
import tensorflow.keras.backend as K

from mldisasm.fixes           import fix_output_size
from mldisasm.io.codec        import AsciiCodec
from mldisasm.io.file_manager import FileManager
from mldisasm.model           import Disassembler
from mldisasm.util            import log, prof, refresh_graph

RANDOM_SEED = 1

def parameter_grid(params):
    '''
    Get a list of parameter sets from a set of parameters whose values are lists.
    '''
    keys, values = zip(*sorted(params.items()))
    sizes        = [len(v) for v in values]
    total_size   = np.product(sizes)
    grid         = [None]*total_size
    for i in range(total_size):
        grid[i] = dict()
        idx     = i
        for k, vs, size in zip(keys, values, sizes):
            idx, off   = divmod(idx, size)
            grid[i][k] = vs[off]
    return grid

def cv_split(X, y):
    '''
    Split a training set in half for cross-validation.
    '''
    X_train, X_test = tf.split(X, 2)
    y_train, y_test = tf.split(y, 2)
    return X_train, y_train, X_test, y_test

def fit_model(config, params, file_mgr, model_name, y_codec):
    '''
    Fit a model to a set of parameters and return the loss during cross-validation.
    '''
    # Seed PRNG with a fixed value so each model gets the same sequence of numbers.
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.set_random_seed(RANDOM_SEED)
    # Get a fresh TF graph.
    refresh_graph()
    # Append training callbacks.
    callbacks = []
    if params.get('stop_early', False):
        callbacks.append(keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=params.get('patience', 0)
        ))
    # Create training set generator.
    generator = file_mgr.yield_training(
        model_name,
        y_codec,
        params['batch_size'],
        keras_mode=True
    )
    # Train and validate the model.
    model       = Disassembler(**params)
    num_batches = min(config['gs_records'],config['max_records'])//params['batch_size']
    total_loss  = 0
    total_acc   = 0
    loss        = np.inf
    acc         = -np.inf
    batch_num = 1
    for X, y in generator:
        with prof(
            'Batch {}/{}: {}% accuracy, loss={}',
            batch_num, num_batches, lambda: 100*acc, lambda: loss,
            log_level='info'
        ):
            # Train with cross-validation.
            X_train, y_train, X_test, y_test = cv_split(X, y)
            # Train and cross-validate using the split.
            model.train_on_batch(X_train, y_train)
            metrics = model.test_on_batch(X_test, y_test)
            # Extract metrics.
            if model.metrics_names == ['acc','loss']:
                acc, loss = metrics
            elif model.metrics_names == ['loss','acc']:
                loss, acc = metrics
            else:
                raise ValueError('Unrecognised metrics names: {}'.format(','.join(model.metrics_names)))
            # Since we are only using 1% of the total training set, we don't want to "waste" any examples, so we train
            # on the validation samples. This effectively doubles the training set without having to load twice as many
            # examples.
            model.train_on_batch(X_test, y_test)
            # Refresh the graph each ten batches to prevent TF slowdown.
            if batch_num % 10 == 0:
                model = refresh_graph(model=model, build_fn=Disassembler, **params)
            if batch_num >= num_batches:
                break
            total_loss += loss
            total_acc  += acc
            batch_num += 1
    # Return the average loss and accuracy.
    return total_loss/batch_num, total_acc/batch_num

def select_params(config, file_mgr, model_name, y_codec):
    '''
    Select hyperparameters by gridsearch with cross-validation.
    '''
    log.info('Selecting hyperparameters')
    grid = parameter_grid(config['grid'])
    if len(grid) == 0:
        log.error('No parameters to tune. Stopping.')
        exit(0)
    fit_num     = 1
    num_fits    = len(grid)
    best_params = None
    best_acc    = -np.inf
    loss        = 0
    acc         = 0
    for grid_params in grid:
        with prof(
            'Fit grid {} with {}% accuracy, loss={}', fit_num, lambda: 100*acc, lambda: loss, 4,
            log_level='info',
            start_msg='Fitting grid {} of {} with parameters {}'.format(fit_num, num_fits, grid_params)
        ):
            params = dict(config['model'])
            params.update(grid_params)
            loss, acc = fit_model(config, params, file_mgr, model_name, y_codec)
            # Select model by accuracy.
            if acc > best_acc:
                best_acc = acc
                best_params = params
            fit_num += 1
    assert best_params is not None
    log.info('Best accuracy was {}% with parameters {}'.format(round(best_acc*100, 2), best_params))
    return best_params

def read_command_line():
    '''
    Get the model name from the command-line.
    '''
    if len(sys.argv) != 2:
        print(__doc__.format(sys.argv[0]), file=sys.stderr)
        exit(1)
    return sys.argv[1]

if __name__ == '__main__':
    # Read command-line args.
    model_name = read_command_line()
    # Start file manager & logging.
    tf.logging.set_verbosity(tf.logging.INFO)
    file_mgr = FileManager()
    log.init(file_mgr.open_log())
    # Train a model.
    try:
        # Load configuration and set TF device.
        config  = file_mgr.load_config()
        tokens  = file_mgr.load_tokens()
        y_codec = AsciiCodec(config['seq_len'], config['mask_value'], tokens)
        # Apply output_size workaround.
        fix_output_size(config, tokens)
        # Find and save hyperparameters.
        K.set_learning_phase(1)
        config['model'] = select_params(config, file_mgr, model_name, y_codec)
        del config['grid']
        file_mgr.save_config(config)
    except Exception as e:
        log.debug('====================[ UNCAUGHT EXCEPTION ]====================')
        log.error('Uncaught exception \'{}\': {}'.format(type(e).__name__, str(e).split('\n')[0]))
        log.error('See the log at {} for details.'.format(file_mgr.log_file_path))
        log.debug('Exception Traceback:\n{}'.format(''.join(tb.format_tb(e.__traceback__))))
        exit(1)
