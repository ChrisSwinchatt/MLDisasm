#!/usr/bin/env python3

'''
Usage: {0} <model>
'''

import copy
import os
import sys
import traceback as tb
import warnings

import numpy as np

# Filter out debug messages from TF.
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

# Ignore warnings generated by using a different NumPy version.
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

if __name__ == '__main__':
    print('*** Starting up...')

import tensorflow               as tf
import tensorflow.keras.backend as K

import mldisasm.benchmarks.profiling as     profiling
from   mldisasm.benchmarks.profiling import prof
import mldisasm.io.log               as     log
from   mldisasm.io.file_manager      import FileManager
from   mldisasm.model                import make_disassembler

def parameter_grid(params):
    '''
    Generate a list of parameter sets from a set of parameters whose values are lists.
    '''
    keys, values = zip(*sorted(params.items()))
    sizes        = [len(v) for v in values]
    size         = np.product(sizes)
    grid         = np.empty(size, dtype=dict)
    for i in range(size):
        grid[i] = dict()
        for k, vs, size in zip(keys, values, sizes):
            _, j = divmod(i, size)
            grid[i][k] = vs[j]
    return grid

def cv_split(X, y):
    '''
    Split a training set in half for cross-validation.
    '''
    X_train, X_test = tf.split(X, 2)
    y_train, y_test = tf.split(y, 2)
    return X_train, y_train, X_test, y_test

def train_model(config, X, y):
    '''
    Train a model by grid-search.
    '''
    grid = parameter_grid(config['grid'])
    X_train, y_train, X_test, y_test = cv_split(X, y)
    fit_num     = 1
    num_fits    = len(grid)
    best_model  = None
    best_params = None
    best_loss   = np.inf
    for grid_params in grid:
        log.info('Fitting grid {} of {}'.format(fit_num, num_fits))
        with prof('Trained CV fit'):
            # Get the parameters and create the model. We use config['model'] for default values for parameters which
            # aren't overridden in config['grid'].
            params = copy.deepcopy(config['model'])
            params.update(grid_params)
            model = make_disassembler(**params)
            # Perform cross-validation.
            history = model.fit(
                X_train,
                y_train,
                steps_per_epoch=1,
                epochs=params['epochs'],
                validation_data=(X_test,y_test)
            )
            loss = history['val_loss']
            if loss < best_loss:
                best_loss   = loss
                best_model  = model
                best_params = params
            fit_num += 1
    assert best_model  is not None
    assert best_params is not None
    log.info('Model with loss={} chosen, best params were:'.format(best_loss))
    log.info('{}'.format(best_params))
    return best_model

def load_datasets(model_name, config, file_mgr):
    '''
    Load training and token sets.
    '''
    log.info('Loading training set')
    X, y   = file_mgr.load_training(model_name)
    tokens = file_mgr.load_tokens(**config)
    return X, y, tokens

def select_device(config):
    '''
    Select a TensorFlow device according to configuration.
    '''
    log.info('Checking TensorFlow device configuration (this can take some time)')
    preferred = config['preferred_device']
    fallback  = config['fallback_device']
    if 'gpu' in preferred.lower() and not tf.test.is_gpu_available():
        if fallback is None:
            log.error('Preferred device \'{}\' is not available and no fallback device was specified, stopping.')
            exit(1)
        log.warning('Preferred device \'{}\' is not available, falling back to \'{}\''.format(
            preferred,
            fallback
        ))
        return fallback
    log.info('Preferred TensorFlow device \'{}\' is available'.format(preferred))
    return preferred

def start_training(model_name, file_mgr):
    '''
    Start training process.
    '''
    # Load configuration and set TF device.
    config = file_mgr.load_config()
    select_device(config)
    # Initialise profiler.
    profiling.init(config['prof_time'], config['prof_mem'])
    # Load datasets, train & save model.
    with tf.Session() as session:
        X, y, _ = load_datasets(model_name, config, file_mgr)
        K.set_session(session)
        K.set_learning_phase(1)
        session.run(tf.global_variables_initializer())
        model = train_model(config, X, y)
        file_mgr.save_model(model, model_name)

def read_command_line():
    '''
    Get the model name from the command-line.
    '''
    if len(sys.argv) != 2:
        print(__doc__.format(sys.argv[0]), file=sys.stderr)
        exit(1)
    return sys.argv[1]

if __name__ == '__main__':
    # Read command-line args.
    model_name = read_command_line()
    # Start file manager & logging.
    file_mgr = FileManager()
    log.init(file_mgr.open_log())
    # Train a model.
    try:
        start_training(model_name, file_mgr)
    except Exception as e:
        log.debug('====================[ UNCAUGHT EXCEPTION ]====================')
        log.error('Uncaught exception \'{}\': {}'.format(type(e).__name__, str(e).split('\n')[0]))
        log.error('See the log at {} for details.'.format(file_mgr.log_file_path))
        log.debug('Exception Traceback:\n{}'.format(''.join(tb.format_tb(e.__traceback__))))
        exit(1)
