#!/usr/bin/env python3

'''
Usage: {0} <model>
'''

import sys
import warnings

# Ignore warnings generated by using a different NumPy version.
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

import tensorflow as tf

import mldisasm.io.log as log
from   mldisasm.io.file_manager    import FileManager
from   mldisasm.model.disassembler import Disassembler

def train_model(tset, config):
    with tf.device('/gpu:0'):
        # Create a model.
        model = Disassembler(**config['model'])
    num_examples = len(tset)

if __name__ == '__main__':
    # Read command-line.
    if len(sys.argv) != 2:
        print(__doc__.format(sys.argv[0]), file=sys.stderr)
        exit(1)
    model_name = sys.argv[1]
    # Set up logging & file manager, load config and open training set.
    file_mgr = FileManager()
    log.init(file_mgr.open_log())
    config   = file_mgr.load_config(model_name)
    tset     = file_mgr.open_training(model_name, shuffled=True)
    # Train on each pair of files in the set.
    with tf.device('/gpu:0'):
        tf.enable_eager_execution()
        # Create the model.
        model = make_disassembler(**config)
        num_examples = len(tset)
        batch_size   = config['batch_size']
        if batch_size < 1:
            batch_size = 1
        for _ in range(num_examples):
            batch = []
            for _ in range(batch_size):
                batch.append(next(tset))
            batch = tf.stack(batch)
            for example in tset:
                pass
            file_mgr.save_model(model, model_name)
